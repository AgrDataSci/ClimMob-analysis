---
title: "`r paste('Analysis Report of ClimMob project ', projname)`"
author: "Automated Report by ClimMob"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  word_document:
    reference_docx: word_style_template.docx
  html_document:
    toc: true
  pdf_document:
    toc: true
bibliography: ["climmob.bib"]
---

```{r setup_opts, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      error = FALSE,
                      message=FALSE,
                      warning = FALSE)
```

You are reading a report generated by ClimMob. This is a software package to analyze data generated by citizen science or crowdsourcing.

# Introduction  

In agriculture, the local environmental conditions determine to a large degree which technological solutions are the most suitable. In dry soils, for example, drought-resistant crop varieties will outperform other varieties, but in wet soils these same varieties may do worse than most. Not only drought, but an entire range of problems including excessive heat, floods, new pests and diseases tend to intensify under climate change. This multitude of limiting factors requires multiple technological solutions, tested in diverse environments. 
 
Citizen science is based on the cooperation of citizen scientist or observers (paid or unpaid). Researchers assign microtasks (observations, experiments) that, once completed and gathered, contribute with a great amount of information to science. One of the advantages of citizen science is that agricultural researchers can get access to many environments by crowdsourcing their experiments. As farmers contribute with their time, skills and knowledge to the investigation, researchers are able to do more tests than in a traditional setup. Also citizen scientists acquire new knowledge, abilities and information useful for future challenges of their work.

## ClimMob

The primary goal of ClimMob is to help farmers adapt to variable and changing climates. ClimMob was created as part of Bioversity International's research in the CGIAR Research Programme on Climate Change, Agriculture, and Food Security (CCAFS). It serves to prepare and analyze citizen science experiments in which a large number of farmers observe and compare different technological options under a wide range environmental conditions [@vanEtten2019tricot]. 
 
ClimMob software assigns a limited number of items (typically 3 crop varieties or agricultural practices) to each farmer, who will compare their performance. Each farmer gets a different combination of items drawn from a much larger set of items. Comparisons of this kind are thought to be a very reliable way to obtain data from human observers. Once the results of the microtasks have been collected, ClimMob builds an image of the whole set of assigned objects, combining all observations. ClimMob not only reconstructs the overall ordering of items, but also takes into account differences and similarities between observers and the conditions under which they observe. It assigns similar observers to groups that each corresponds to a different preference profile. Groups are created on the basis of whichever variables which have been collected, that are found to be significantly linked to the preferences.
 
ClimMob uses Plackett-Luce models to analyze ranking data [@Turner2020]. It automatically generates analytical reports, as well as individualized information sheets for each participant. 
 

## How to cite

If you publish any results generated with ClimMob, you should cite a number of articles as the package builds on various contributions. Van Etten [-@vanEtten2019tricot] introduced the crowdsourcing philosophy behind ClimMob. It is important to mention that ClimMob is implemented in `R`, a free, open-source analysis software [@RCoreTeam]. Methodologically, if you report on the tree results, you should mentioned that ClimMob applies the Plackett-Luce model published by Turner et al. [-@Turner2020]. To cite ClimMob itself, mention van Etten et al. [-@climmob].

# Section 1: Headline Results

Overall there were `r nranker` '`r rankers` participating in this study. Each `r ranker` assessed `r ncomp` different `r options` and ranked them in order of their overall preference. In addition they also provided rankings for `r nothertraits` additional traits: 

```{r traits}

kable(data.frame("Short name" = pars$chars$char,
                 "Question"   = pars$chars$char_full,
                 check.names = FALSE), 
      align = "l")

```


Table 1 provides a list of the `r options` assessed within this trial, with the frequency and percentage of `r rankers` who assessed each `r option`.

```{r itemtable}
kable(itemtable, 
      caption = paste0("Table 1: Frequency of ", options," assessed"),
      align = "l")
```

## Overall Differences in Rankings

```{r statistical-diff}
if (!is.na(ps[1])) {
  
  if (ps[1] < sig_level) {
    line1 <- paste0("Overall there were statistically significant differences found",
                    " in the rankings of ", options ," in the overall ranking (p=",ptab$p.value[1],
                    "). The best ranked ", options ," overall were ",ptab$`Best Ranked`[1])
  }
  
  if (ps[1] >= sig_level) {
    line1 <- paste0("Overall there were no statistically significant differences found",
                    " in the rankings of ", options ," in the overall ranking (p=",
                    ptab$p.value[1],")")
  }
  
}

if (is.na(ps[1])) {
  line1 <- "No statistical model was able to be fitted on this data"
}

if (any(na.omit(ps[2:length(ps)]) < sig_level)){
  line1 <- paste(line1,". Statistically significant differences were also found in the trait(s)",
                 paste(trait_full[ps[2:length(ps)] < sig_level], collapse=", "))
}

```

`r  paste(line1, collapse='\n')`

A summary of the p-values testing the hypothesis that there exist differences in the rankings within each of the Plackett-Luce models fitted for each of the assessed traits, and the list of `r options` which were significantly highest and lowest ranked overall, are summarised in Table 1.1.

```{r}
kable(ptab, 
      caption = paste0("Table 1.1: Summary of differences found in ",options," by trait"))
```

See Section 3 for further details.

## Effect of covariates

```{r eff-expvar}
if (length(na.omit(outtabs[[1]]$p.value)) > 0) {
  if (any(na.omit(outtabs[[1]]$p.value) < sig_level)) {
    line2 <- paste0(length(siglist), " of the variables tested were found to have a statistically",
                   " significant relationship to the overall ranking. These variable(s) were:",  
                   paste(paste0(siglist," (p=",outtabs[[1]][siglist,"p"],")"),collapse=", "))
    
  }
  if( !any(na.omit(outtabs[[1]]$p.value) < sig_level)) {
    line2 <- paste0("None of the variables tested were found to have a statistically significant",
                    " relationship to the overall ranking at the ", sig_level*100, 
                    "% significance level.")
  }
}
if (length(na.omit(outtabs[[1]]$p.value)) == 0) {
  line2 <- "No models to test covariates were successfully fitted on this analysis."
}

```

`r  paste(line2, collapse='\n')`

A summary of the univariate signifance levels for all covariates that were able to be tested is shown in Table 1.2.1.
```{r summary-univariate}

kable(uni_sum[,c(5,4)],
      caption = paste0("Table 1.2.1: Summary of univariate p-values for first split in ",
                       "Plackett-Luce tree model for the overall ranking"))


```

```{r no-expvar-used}
if (length(expvar_dropped) > 0) {
  stopmessage <- paste("The variable(s)",
                      paste(expvar_dropped, collapse = ", "), 
                      "were not able to be included within the analysis.")
}

if (length(expvar_dropped) == 0) {
  stopmessage <- ""
}

```

`r  paste(stopmessage, collapse='\n')`

```{r}
if (length(siglist) > 0) {
  line3 <- paste0("Table 1.2.2 shows which ",options,
                  " where identified as the best and worst ranked in the subgroups identified by",
                  " including explanatory variables into the overall rankings.")
}
if (length(siglist) == 0) {
  line3<-""
}
```

`r  paste(line3, collapse='\n')`

```{r subgroups-found}
if(length(siglist)>0){
  kable(node_summary, 
        caption = paste0("Table 1.2.2: Summary of different subgroups identified",
                         " by multivariate Plackett-Luce tree model"))
}

```

See Section 4 for further details.

## Relationships between traits

Table 1.3 shows, for each trait in the study, the frequency with which the rankings matched with the overall ranking.

The trait which had the strongest relationship with the overall ranking was `r  strongest_link[1]`. Overall the rankings for `r  strongest_link[1]` matched the rankings for the overall ranking `r strongest_link[2]`% of the time.

The trait which had the weakest relationship with the overall ranking was `r  weakest_link[1]`. Overall the rankings for `r  weakest_link[1]` matched the rankings for the overall ranking only `r  weakest_link[2]`% of the time.



```{r agree-table}
knitr::kable(agreement_table[,1:2],
             caption="Table 1.3: Relationship between individual trait assessment and overall assesment")
```

See Section 5 for further details.


# Section 2: Data Summary and Exploratory Analysis of Traits

## Assessment of `r options` 

Exploratory analysis within the following section summarises results from the data directly. Given the structure of a ClimMob trail, where each `r ranker` only assesses `r ncomp` of the `r nitems` possible `r options` these results may be skewed if certain `r options` were randomly assigned to face worse `r options` than others. This is particularly a potential issue within a smaller trial, as due to the randomisation process the potential for an unbalanced assignment decreases as the sample size increases. Results from other sections, and in the overall summary use Plackett-Luce models [@Turner2020], to adjust for any imbalance.

### Overall  

Overall performance of each of the `r options` is summarised in Table 2.1. 

```{r fav_table}
knitr::kable(fav2,
             row.names = FALSE,
             caption = "Table 2.1 - Summary of overall performance")
```


This shows the percentage of `r rankers` who assessed the `r options` as their most preferred of the `r ncomp` `r options` they were provided, the percentage of `r rankers` who included the `r option` as their least preferred, the percentage of 'head to head contests' for which the `r option` won and the net favourability score. A score of +100 indicates the `r option` won all 'contests' it was involved in, a score of 0 indicates an equal number of wins and losses, a score of -100 indicates the variety lost all contests. 

*Figure 2.1*
```{r fav_plot}
plot(fav1) + 
  ggtitle("Net Favourability Score for Overall Performance") + 
  xlab(Option)

favsentence_i <- which(fav1$fav_score < max(fav1$fav_score) & fav1$fav_score >= 50)

favsentence <- ifelse(length(favsentence_i) > 0, 
                      paste0("Other ", options," with strong positive rankings (> 50) were ", 
                             paste(fav1$items[favsentence_i], collapse=", "),"."),
                      "")
```

The `r option` `r fav1[[1,1]]` was the most preferred `r option` overall being ranked highest by `r fav1[[1,3]]` of the `r fav1[[1,2]]` `r rankers` who assessed this `r option`. 

`r favsentence`

### Other Traits  

Net favourability scores are shown below for the other traits assessed in this study.

```{r, traitsplots}
trait_summaries = NULL

other_traits_list <- trait_list[other_traits]

if(length(other_traits_list) > 0) {
  for(i in seq_along(other_traits_list)){
    trait_summaries<-c(trait_summaries,
                       knitr::knit_child("trait_analysis.Rmd", 
                                         quiet=TRUE))
}  
}

```

```{r traitinclude, echo=FALSE, results="asis", message=FALSE, warning=FALSE}
cat(paste(trait_summaries, collapse = '\n'))
```


## Pairwise Contests 

Appendix B contains the full data for each pairwise comparison of the varieties, which is summarised in the plots below.

### Overall 

Figure 2.3 shows the outcomes of all pairwise contests between the `r options` included in the trial. Each panel shows the performance of one `r option` against all the other `r options`, and shows the percentage of the times in which the panelled `r option` was ranked above the `r options` shown as bars. The most preferred `r option` is shown in the top left panel and the least preferred is shown in the bottom right panel

*Figure 2.3*
```{r contests bars,fig.height=12,fig.width=10}
plot(contests[[2]]) +
  ggtitle("Head to Head Performance of All Varieties Using Overall Trait Preference") +
  ylab("Contests Preferred (%)")

```

### Other Traits 

Results from the pairwise contests of the other individual traits assessed are shown below. 

```{r, pairsplots}
trait_pairs1 = NULL
if(length(other_traits_list) > 0) {
  for(i in seq_along(other_traits_list)){
    trait_pairs1 <- c(trait_pairs1,
                      knitr::knit_child("trait_pairs.Rmd", 
                                        quiet=TRUE))
}
}
```

```{r traitincludepairs, echo=FALSE, results="asis", message=FALSE, warning=FALSE}
cat(paste(trait_pairs1, collapse = '\n'))
```


## Relationship between traits  

Table 2.5 shows the relationship between the individual trait rankings and the overall rankings. Complete agreement represents the percentage of respondents for which the ranking of the `r ncomp` `r options` in respect to the trait is an exact match to the overall ranking. Best and worst agreement represents the percentage for which the best and worst `r option` for the trait matched the overall best and worst. Complete ranking agreement shows the proportion of correlation on the full ranking with the overall performance as baseline using the Kendall correlation coefficient [@Kendall1938].

```{r}
kable(agreement_table,
      caption = "Table 2.5: Relationship between individual trait assessment and overall assessment")
```

*Figure 2.5*
```{r correspondence, fig.width=9, fig.height=6}
plot(agreement)
```

Overall the trait which had the strongest relationship with the overall ranking was `r agreement[[which.max(agreement$kendall), "labels"]]`, with identical rankings being given as the overall ranking `r paste0(round(max(agreement$kendall), 2),"%")` of the time.


# Section 3: Data Summary & Exploratory Analysis of Explanatory Variables  


```{r exp-var-table}
kable(data.frame("Short name" = expvar_list$expvar, 
                 "Question" = expvar_list$expvar_full, 
                 check.names = FALSE))
```

## Overall Ranking

Table 4.1 shows the results from the likelihood ratio test from the Plackett-Luce model for overall rankings of the different `r options`. The hypothesis being tested is that there is no difference in the assessments of any of the different `r options`.

```{r, aov1}
x1 <- anova.PL(mod_overall)
p <- x1[2,5]
x1[,6] <- stars.pval(x1[, 5])
x1[,5] <- format.pval(x1[, 5])
names(x1)[6] <- ""
x1[2, "model"] <- "Overall"

kable(x1, 
      caption = "Table 4.1: Likelihood ratio test results from overall model")
```

```{r,pval_sentence}
if (p < 0.001) {
  message1 <- paste0("p=", format.pval(p), 
                    ". This suggests that there is strong evidence of ",
                    "a difference in the rankings between ",
                    options)
}

if (p<0.01 & p >0.001) {
  message1 <- paste0("p=", format.pval(p), 
                     ". This suggests that there is evidence of a difference between ",
                     options)
}
if (p>0.01 & p <0.05) {
  message1 <- paste0("p=", format.pval(p), 
                    ". This suggests that there is some evidence of a difference between ",
                    options)  
}
if (p>0.05) {
  message1 <- paste0("p=", format.pval(p),
                    ". This suggests that there is not enough evidence to conclude that",
                    " there are differences between",
                    options)  
}
```

`r message1`

Figure 4.1 shows the estimates of the model coefficients with `r ci_level*100`% confidence intervals. The purpose of this graph is to be able to best distinguish between the relative strength of each of the `r options` assessed. As such the coefficient estimates themselves are not directly interpretable, but it can be concluded that a higher value for the coefficient indicates that a variety has been more preferred. The `r ci_level*100`% confidence width is chosen so that non-overlapping confidence intervals could be interpreted as indicating significant differences at the `r sig_level*100`% significance level. This may not match exactly with the mean separation groupings, as these groupings also take into account multiple testing through the Benjamini and Hochberg adjustment.

Mean separation analysis was also conducted to indicate, using letters, which `r options` are significantly more preferred than others: when `r options` have at least one letter in common, there is not enough evidence from the experiment to be confident about their relative order of preference at the `r sig_level*100`% significance level. 

```{r est_plot, fig.width=7,fig.height=7, fig.cap="Figure 4.1 - Overall ranking: model coefficients and mean separation.\n"}
plot.multcompPL(model_summaries, level = ci_level) +
  ggtitle(paste0("Overall preference estimates with ", 
                 ci_level*100, "% confidence intervals"))
```

The same information as Figure 4.1 is shown in Table 4.2 below


```{r est_table}
ms <- model_summaries[, c("estimate","quasiSE",".group")]
names(ms) <- c("Estimate","quasiSE","Group")

kable(ms, digits = 2,
      caption = paste0("Table 4.2 - Model coefficients and mean separation of ",
                       options," at ",
                       100*sig_level,"% level with ",ci_adjust," adjustment"))
```


Table 4.3 and Figure 4.2 use the coefficients from the model to estimate the probability of each `r option` being considered to be the top ranked `r option` in a direct comparison between all of the possible `r options`

```{r}
kable(worthscaled[, -2],
      caption = "Table 4.3: Percentage probability of being the highest ranked overall.")
```

```{r rankedfirst, fig.cap="Figure 4.2 - Overall Ranking: Probability of Being The Highest Ranked Overall.\n"}
ggplot(data=worthscaled, aes(y = Worth, fill = Worth, group = worthscaled[,1], x = 1)) +
  geom_bar(stat="identity",col="black", show.legend = FALSE) +
  theme_void() +
  geom_text(aes(y=cumsum(rev(Worth))-rev(Worth)/2, size=rev(Worth),
                label=rev(paste0(worthscaled[,1],": ", round(Worth*100, 2),"%"))),
            show.legend = FALSE,fontface=2) +
  ggtitle(paste("Probability that",option,"is the\nhighest ranked overall")) +
  scale_fill_gradient2(low= "white", high = "forestgreen", midpoint= 1/nrow(worthscaled))  +
  scale_size_continuous(range=c(0.5,10),limits = c(0,1))
```

# Section 5: Plackett-Luce Models of Other Traits

```{r, traitmodels}
model_traits <- NULL
if (length(other_traits_list) > 0) {
  for (i in seq_along(other_traits_list)){
    model_traits<-c(model_traits,
                    knitr::knit_child("trait_models.Rmd", quiet=TRUE))
}
}

```


```{r traitmodinclude, echo=FALSE, results="asis", message=FALSE, warning=FALSE}
cat(paste(model_traits, collapse = '\n'))
```


# Section 6: Plackett-Luce models with explatory variables  

## Overall ranking  

A recursive partitioning method [@Strobl2009] was used to determine which of the explanatory variables, if any, had significant relationships with the rankings. This approach identifies sub-groups in the data for which the rankings of the different varieties are significantly different to each other. Table 4.1 shows the p-values for each of the covariates tested, one-by-one, showing whether or not the covariate could be used to define sub-groups with significantly different rankings. 

```{r univar_pval}
kable(uni_sum[,c(5,4)],
      caption = paste0("Table 6.1: Univariate p-values for first split in ",
                       "Plackett-Luce tree model for the overall ranking"))
```


Figure 6.1 shows the partitioning of the rankings based on the most significantly different sub-groups which could be identified from the data using a `sig_level*100`% significance level. At the top of the tree is the full dataset, then working down through the different levels of the tree shows the combinations of variables which define each subgroup. The model parameters are shown for the final subgroups ("terminal nodes") in the plots at the bottoms of the tree. 


```{r pltree, asis=TRUE,fig.height=8,fig.width=10, fig.cap="Figure 6.1 - Overall ranking of Plackett-Luce tree."}
if (length(tree_f)==1) {
  nofig1 <- "No significant explanatory variables identified."
}else{
  nofig1 <- ""
}

plot(tree_f)
```
`r nofig1`

The highest and lowest performing `r option` within each sub-group is identified within Table 6.2.

```{r node_summary}
if (length(siglist) > 0) {
  kable(node_summary,
        caption = "Table 6.2 - Summary of performance in each tree node.")
}
```

The model coefficient estimates, along with `r ci_level*100`% confidence intervals are provided in Figure 6.2. This will help identification of which `r options` were better suited to particular sub-groups identified by the analysis.

```{r,fig.width=8,fig.height=8, fig.cap="Figure 6.2 - Coefficient estimates within each identified terminal node\n"}
if(length(tree_f)>1){
ggplot(data = coefs_t, 
       aes(x = term, 
           y = ctd,
           ymax = ctd+1.40*quasiSE,
           ymin = ctd-1.40*quasiSE,
           col = Label)) +
  geom_point(position = position_dodge(width = 0.3), size = 1) +
  geom_errorbar(position = position_dodge(width = 0.3), width = 0) +
  coord_flip() +
  scale_color_discrete() +
  geom_text(aes(label=.group),
            size = 3,
            fontface = 2,
            nudge_x = rep(c(-0.3,0.5), each = nlevels(coefs_t$term))) +
  ylab("") +
  xlab(Option) +
  ggtitle(paste("Terminal node parameter estimates for",
                "overall rankings."))
}

```
`r nofig1`

Table 6.3 outlines the p-values for each covariate at each of the nodes in the tree, outlining whether an additional significant split could be determined from within the existing sub-group at that node.

```{r}
z<-NULL
for(i in 1:length(outtabs)){
  if (ncol(outtabs[[i]]) > 3) {
    ot <- data.frame(Variable = rownames(outtabs[[i]]),
                     outtabs[[i]])
    z <- rbind(z, ot)
  }
}
if (length(z) > 0) {
  nodemessage <- ""
  kable(z[,c(1:3,5)],
        caption="Table 6.3: p-values for effect of each covariate at each node",
        row.names = FALSE)
}
if (length(z)==0) {
  nodemessage<-"No covariates were able to be assessed statistically."
}

```
`r nodemessage`


# References