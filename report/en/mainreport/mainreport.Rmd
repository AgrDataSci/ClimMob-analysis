---
title: "`r paste('Analysis of ClimMob project ', projname)`"
author: "Automated Report by ClimMob"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  word_document:
    reference_docx: word_style_template.docx
  html_document:
    toc: true
  pdf_document:
    toc: true
bibliography: ["climmob.bib"]
csl: citation_style.csl
---

```{r setup_opts, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      error = FALSE,
                      message=FALSE,
                      warning = FALSE)
```

You are reading a report generated by ClimMob. This is a software package to analyze data generated by citizen science or crowdsourcing.

# Introduction  

In agriculture, the local environmental conditions determine to a large degree which technological solutions are the most suitable. In dry soils, for example, drought-resistant crop varieties will outperform other varieties, but in wet soils these same varieties may do worse than most. Not only drought, but an entire range of problems including excessive heat, floods, new pests and diseases tend to intensify under climate change. This multitude of limiting factors requires multiple technological solutions, tested in diverse environments. 
 
Citizen science is based on the cooperation of citizen scientist or observers (paid or unpaid). Researchers assign microtasks (observations, experiments) that, once completed and gathered, contribute with a great amount of information to science. One of the advantages of citizen science is that agricultural researchers can get access to many environments by crowdsourcing their experiments. As farmers contribute with their time, skills and knowledge to the investigation, researchers are able to do more tests than in a traditional setup. Also citizen scientists acquire new knowledge, abilities and information useful for future challenges of their work.

## ClimMob

The primary goal of ClimMob is to help farmers adapt to variable and changing climates. ClimMob was created as part of Bioversity International's research in the CGIAR Research Programme on Climate Change, Agriculture, and Food Security (CCAFS). It serves to prepare and analyze citizen science experiments in which a large number of farmers observe and compare different technological options under a wide range environmental conditions [@vanEtten2019tricot]. 
 
ClimMob software assigns a limited number of items (typically 3 crop varieties or agricultural practices) to each participant, who will compare their performance. Each participant gets a different combination of items drawn from a much larger set of items. Comparisons of this kind are thought to be a very reliable way to obtain data. Once the results of the microtasks have been collected, ClimMob builds an image of the whole set of assigned objects, combining all observations. ClimMob not only reconstructs the overall ordering of items, but also takes into account differences and similarities between participants and the conditions under which they observe (e.g. socio-economic and plot environmental characteristics). It assigns similar participants to groups that each corresponds among different group profiles. Groups are created on the basis of whichever items which have been collected, that are found to be significantly linked to the observed rankings.
 
ClimMob uses Plackett-Luce models to analyze ranking data with the R [@RCoreTeam] package 'PlackettLuce' [@Turner2020]. It automatically generates analytical reports, as well as individualized information sheets for each participant using the R packages 'knitr' [@knitr] and 'rmarkdown' [@rmarkdown]. Organizing the data relies on packages 'ClimMobTools' [@climmobtools], 'gosset' [@gosset], 'gtools' [@gtools], 'jsonlite' [@jsonlite], 'partykit' [@partykit], 'psychotools' [@psychotools] and 'qvcalc' [@qvcalc]. Summaries and data visualization are supported by packages 'ggplot2' [@ggplot2], 'igraph' [@igraph] and 'pls' [@pls].
 

## How to cite

If you publish any results generated with ClimMob, you should cite a number of articles as the package builds on various contributions. Van Etten et al. (2019) [@vanEtten2019tricot] introduced the crowdsourcing philosophy behind ClimMob. It is important to mention that ClimMob is implemented in R, a free, open-source analysis software [@RCoreTeam]. Methodologically, if you report on the Plackett-Luce tree results, you should mentioned that ClimMob applies the Plackett-Luce model published by Turner et al. 2020 [@Turner2020]. To cite ClimMob itself, mention van Etten et al. (2020) [@climmob].

\pagebreak

# Section 1: Headline results

Overall there were `r nranker` `r rankers` contributing to this project. Each `r ranker` assessed `r ncomp` different `r options` and ranked them in order of its overall performace. In addition they also provided rankings for `r nothertraits` additional characteristics: 

```{r characteristics}

kable(data.frame("Short name" = pars$chars$char,
                 "Question"   = pars$chars$char_full,
                 check.names = FALSE), 
      align = "l")

```

\

Table 1 shows the `r options` assessed within this project, with the frequency and percentage of `r rankers` who assessed each `r option`. If the `r options` has large names (> 10 characters) an abbreviation was applied across the figures in this report.

```{r itemtable}
kable(itemtable, 
      caption = paste0("Table 1. Frequency of ", options," assessed."),
      align = "l")
```

\

This is how the different `r options` were connected.

```{r network, dpi=dpi, fig.height=7, fig.width=7, fig.cap=paste0("Figure 1. Network representation of ", options, " tested in this project.")}
igraph::plot.igraph(net,
                    edge.arrow.size = 0.5) 
```

\pagebreak

## Overall differences in rankings

```{r statistical-diff}
if (!is.na(ps[1])) {
  
  if (ps[1] < sig_level) {
    line1 <- paste0("There were statistically significant differences found",
                    " in the rankings of ", options ," in the overall performance (p=",ptab$p.value[1],
                    "). The best ranked ", options ," overall were ",ptab$`Best Ranked`[1])
  }
  
  if (ps[1] >= sig_level) {
    line1 <- paste0("There were no statistically significant differences found",
                    " in the rankings of ", options ," in the overall performance (p=",
                    ptab$p.value[1],")")
  }
  
}

if (is.na(ps[1])) {
  line1 <- "No statistical model was able to be fitted on this data"
}

if (any(na.omit(ps[2:length(ps)]) < sig_level)){
  line1 <- paste(line1,". Statistically significant differences were also found in the characteristic(s)",
                 paste(trait_full[ps[2:length(ps)] < sig_level], collapse=", "))
}

```

`r  paste(line1, collapse='\n')`

A summary of the p-values testing the hypothesis that there exist differences in the rankings within each of the Plackett-Luce models fitted for each of the assessed characteristics, and the list of `r options` which were significantly highest and lowest ranked overall, are summarised in Table 1.1.

\

```{r}
kable(ptab, 
      caption = paste0("Table 1.1. Summary of differences found in ",options," by characteristic."))
```


## Effect of covariates

```{r eff-expvar}
if (length(na.omit(outtabs[[1]]$p.value)) > 0) {
  if (any(na.omit(outtabs[[1]]$p.value) < sig_level)) {
    line2 <- paste0(length(siglist), " of the variables tested were found to have a statistically",
                   " significant relationship to the overall performance. These variable(s) were:",  
                   paste(paste0(siglist," (p=",outtabs[[1]][siglist,"p"],")"),collapse=", "))
    
  }
  if( !any(na.omit(outtabs[[1]]$p.value) < sig_level)) {
    line2 <- paste0("None of the variables tested were found to have a statistically significant",
                    " relationship to the overall performance at the ", sig_level*100, 
                    "% significance level.")
  }
}
if (length(na.omit(outtabs[[1]]$p.value)) == 0) {
  line2 <- "No models to test covariates were successfully fitted on this analysis."
}

```

`r  paste(line2, collapse='\n')`

A summary of the univariate signifance levels for all covariates that were able to be tested is shown in Table 1.2.1.
```{r summary-univariate}

kable(uni_sum[,c(5,4)],
      caption = paste0("Table 1.2.1. Summary of univariate p-values for first split in ",
                       "Plackett-Luce tree model for the 'overall performance'."))


```

\

```{r no-expvar-used}
if (length(expvar_dropped) > 0) {
  stopmessage <- paste("The variable(s)",
                      paste(expvar_dropped, collapse = ", "), 
                      "were not able to be included within the analysis.")
}

if (length(expvar_dropped) == 0) {
  stopmessage <- ""
}

```

`r  paste(stopmessage, collapse='\n')`

```{r}
if (length(siglist) > 0) {
  line3 <- paste0("Table 1.2.2 shows which ",options,
                  " where identified as the best and worst ranked in the subgroups identified by",
                  " including explanatory variables into the 'overall performance'.")
}
if (length(siglist) == 0) {
  line3<-""
}
```

`r  paste(line3, collapse='\n')`

```{r subgroups-found}
if(length(siglist)>0){
  kable(node_summary, 
        caption = paste0("Table 1.2.2. Summary of different subgroups identified",
                         " by multivariate Plackett-Luce tree model."))
}

```


## Relationships between characteristics

Table 1.3 shows, for each characteristic in the project, the frequency with which the rankings matched with the overall performance.

The characteristic which had the strongest relationship with the overall performance was `r  strongest_link[1]`. Overall the rankings for `r  strongest_link[1]` matched the rankings for the overall performance `r strongest_link[2]`% of the time.

The characteristic which had the weakest relationship with the overall performance was `r  weakest_link[1]`. Overall the rankings for `r  weakest_link[1]` matched the rankings for the overall performance only `r  weakest_link[2]`% of the time.


```{r agree-table}
knitr::kable(agreement_table[,1:2],
             caption="Table 1.3. Relationship between individual characteristics and 'overall performance'.")
```


\pagebreak

# Section 2: Data summary and exploratory analysis of characteristics

## Assessment of `r options` 

Exploratory analysis within the following section summarises results from the data directly. Given the structure of a ClimMob trail, where each `r ranker` only assessed `r ncomp` of the `r nitems` possible `r options` these results may be skewed if certain `r options` were randomly assigned to face worse `r options` than others. This is particularly a potential issue within a smaller trial, as due to the randomisation process the potential for an unbalanced assignment decreases as the sample size increases. Results from other sections, and in the overall summary use Plackett-Luce models [@Turner2020], to adjust for any imbalance.

### Overall performance 

Overall performance of each of the `r options` is summarised in Table 2.1. 


```{r fav_table}
knitr::kable(fav2,
             row.names = FALSE,
             caption = "Table 2.1. Summary of overall performance.")
```

\

This shows the percentage of `r rankers` who assessed the `r options` as the best among the `r ncomp` `r options` they were provided, the percentage of `r rankers` who included the `r option` as their worst, the percentage of 'head to head contests' for which the `r option` won and the net favourability score. A score of +100 indicates the `r option` won all 'contests' it was involved in, a score of 0 indicates an equal number of wins and losses, a score of -100 indicates the `r option` lost all contests. 

```{r fav_plot, dpi=dpi, fig.height=favplot_h, fig.width=6, fig.cap="Figure 2.1. Net favourability score for 'overall performance'."}
plot(fav1) + 
  xlab("") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 10, color = "#000000"),
        axis.text.y = element_text(size = 10, color = "#000000"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```

```{r favsentence}
favsentence_i <- which(fav1$fav_score < max(fav1$fav_score) & fav1$fav_score >= 50)

favsentence <- ifelse(length(favsentence_i) > 0, 
                      paste0("Other ", options," with strong positive rankings (> 50) were ", 
                             paste(fav1$items[favsentence_i], collapse=", "),"."),
                      "")
```

\

The `r option` `r fav1[[1,1]]` was the 'best' `r option` overall being ranked highest by `r round(fav1[[1,3]], 1)`% of the `r fav1[[1,2]]` `r rankers` who assessed this `r option`. 

`r favsentence`

### Other characteristics  

Net favourability scores are shown below for the other characteristics assessed in this project.


```{r, characteristicsplots}
trait_summaries <- NULL

other_traits_list <- trait_list[other_traits]

if(length(other_traits_list) > 0) {
  for(i in seq_along(other_traits_list)){
    trait_summaries<-c(trait_summaries,
                       knitr::knit_child(paste0(fullpath, "/report/",
                                                language, "/mainreport/trait_analysis.Rmd"), 
                                         quiet=TRUE))
}  
}

```

```{r traitinclude, echo=FALSE, results="asis", message=FALSE, warning=FALSE}
cat(paste(trait_summaries, collapse = '\n'))
```

\pagebreak

## Pairwise contests 

\

### Overall performance

Figure 2.3 shows the outcomes of all pairwise contests between the `r options` included in the project. Each panel shows the performance of one `r option` against all the other `r options`, and shows the percentage of the times in which the panelled `r option` was ranked above the other `r options` shown as bars. The most winner `r option` is shown in the top left panel and the least winner is shown in the bottom right panel.

```{r contests_bars, dpi=dpi, fig.height=contest_h, fig.width=9, fig.cap="Figure 2.3. Head to head performance for 'overall performance'."}
if(length(other_traits) > 0){
  plot(contests[[2]]) + 
  labs(y = "", x = "") + 
  theme_minimal() +  
  theme(axis.text.x = element_text(size = 10, color = "#000000"),
        axis.text.y = element_text(size = 10, color = "#000000"),
        strip.text.x = element_text(size = 11, color = "#000000", face = "bold"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
}
```

### Other characteristics 

Results from the pairwise contests of the other individual characteristics assessed are shown below. 

```{r pairsplots}
trait_pairs1 <- NULL
if(length(other_traits_list) > 0) {
  for(i in seq_along(other_traits_list)){
    trait_pairs1 <- c(trait_pairs1,
                      knitr::knit_child(paste0(fullpath, "/report/", 
                                               language, "/mainreport/trait_pairs.Rmd"), 
                                        quiet=TRUE))
}
}
```

```{r traitincludepairs, echo=FALSE, results="asis", message=FALSE, warning=FALSE}
cat(paste(trait_pairs1, collapse = '\n'))
```

## Relationship between characteristics  

Table 2.5 shows the relationship between the individual characteristic rankings and the overall performance. Complete agreement represents the percentage of `r rankers` for which the ranking of the `r ncomp` `r options` in respect to the characteristic is an exact match to the overall performance. Best and worst agreement represents the percentage for which the best and worst `r option` for the characteristic matched the overall best and worst. Complete ranking agreement shows the proportion of correlation on the full ranking with the overall performance as baseline using the Kendall correlation coefficient [@Kendall1938].

\

```{r agreement_table}
kable(agreement_table,
      caption = "Table 2.5. Relationship between individual characteristic and 'overall performance'.")
```

\

```{r correspondence, dpi=dpi,fig.height=agreem_h, fig.width=9, fig.cap="Figure 2.4. Agreement between individual characteristic and 'overall performance'."}
if (length(other_traits) > 0) {
  plot(agreement) +
  theme_minimal() +  
  theme(axis.text.x = element_text(size = 10, color = "#000000"),
        axis.text.y = element_text(size = 10, color = "#000000"),
        strip.text.x = element_text(size = 11, color = "#000000", face = "bold"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  scale_fill_manual(values = c('#66c2a5','#fc8d62','#8da0cb'))
}
```

\

The characteristic which had the strongest relationship with the overall performance was `r agreement[[which.max(agreement$kendall), "labels"]]`, with identical rankings being given as the overall performance `r paste0(round(max(agreement$kendall), 2),"%")` of the time.


# Section 3: Data summary and exploratory analysis of covariates  


```{r expvar_table}
kable(data.frame("Short name" = expvar_list$expvar, 
                 "Question" = expvar_list$expvar_full, 
                 check.names = FALSE),
      caption = "Table 3.1. Covariates used in this project.")
```

\

## Overall performance

Table 3.2 shows the results from the likelihood ratio test from the Plackett-Luce model for overall performance of the different `r options`. The hypothesis being tested is that there is no difference in the assessments of any of the different `r options`.

```{r, aov1}
x1 <- anova.PL(mod_overall)
p <- x1[2,5]
x1[,6] <- stars.pval(x1[, 5])
x1[,5] <- format.pval(x1[, 5])
names(x1)[6] <- ""
x1[2, "model"] <- "Overall"

kable(x1, 
      caption = "Table 3.2. Likelihood ratio test results from fitted Plackett-Luce model with rankings from 'overall performance'.")
```


```{r pval_sentence}
if (p < 0.001) {
  message1 <- paste0("p=", format.pval(p), 
                    ". This suggests that there is strong evidence of ",
                    "a difference in the rankings between ",
                    options)
}

if (p<0.01 & p >0.001) {
  message1 <- paste0("p=", format.pval(p), 
                     ". This suggests that there is evidence of a difference between ",
                     options)
}
if (p>0.01 & p <0.05) {
  message1 <- paste0("p=", format.pval(p), 
                    ". This suggests that there is some evidence of a difference between ",
                    options)  
}
if (p>0.05) {
  message1 <- paste0("p=", format.pval(p),
                    ". This suggests that there is not enough evidence to conclude that",
                    " there are differences between ",
                    options)  
}
```


`r message1`

\

Figure 3.1 shows the estimates of the model coefficients with `r ci_level*100`% confidence intervals. The purpose of this graph is to be able to best distinguish between the relative strength of each of the `r options` assessed. As such the coefficient estimates themselves are not directly interpretable, but it can be concluded that a higher value for the coefficient indicates that a `r option` has been ranked as best more often. The `r ci_level*100`% confidence width is chosen so that non-overlapping confidence intervals could be interpreted as indicating significant differences at the `r sig_level*100`% significance level. This may not match exactly with the mean separation groupings, as these groupings also take into account multiple testing through the Benjamini and Hochberg adjustment.

Mean separation analysis was also conducted to indicate, using letters, which `r options` are significantly more preferred than others; when `r options` have at least one letter in common, there is not enough evidence from the experiment to be confident about their relative order at the `r sig_level*100`% significance level. 

```{r est_plot, dpi=dpi, fig.height=favplot_h, fig.width=5, fig.cap=paste0("Figure 3.1. Model coefficients and mean separation of Plackett-Luce model for 'overall performance' with ", ci_level*100, "% confidence intervals.")}
plot.multcompPL(model_summaries, level = ci_level) + 
  theme_classic() +
  theme(axis.text.x = element_text(size = 10, color = "#000000"),
        axis.text.y = element_text(size = 10, color = "#000000")) 
```

\

The same information as Figure 3.1 is shown in Table 3.3 below.

\

```{r est_table}
ms <- model_summaries[, c("estimate","quasiSE",".group")]
names(ms) <- c("Estimate","quasiSE","Group")

kable(ms, digits = 2,
      caption = paste0("Table 3.3. Model coefficients and mean separation of ",
                       options," with ",
                       100*sig_level,"% level with ",ci_adjust," adjustment."))
```

\

Table 3.4 and Figure 3.2 use the coefficients from the Plackett-Luce model to estimate the probability of each `r option` being considered to be the top ranked `r option` in a direct comparison between all of the possible `r options`.

\

```{r}
kable(worthscaled[, -2],
      row.names = FALSE,
      caption = "Table 3.4. Percentage probability of being the best ranked for 'overall performance'.")
```

\

```{r rankedfirst, dpi=dpi, fig.height=favplot_h, fig.width=5, fig.cap="Figure 3.2. Probability of being the best ranked for 'overall performance'."}
plot_worth_bar(worthscaled, value = 2, group = 1)
```

\

# Section 4: Plackett-Luce models of other characteristics

```{r, characteristicmodels}
model_traits <- NULL
if (length(other_traits_list) > 0) {
  for (i in seq_along(other_traits_list)){
    model_traits<-c(model_traits,
                    knitr::knit_child(paste0(fullpath, "/report/", 
                                             language, "/mainreport/trait_models.Rmd"), 
                                      quiet=TRUE))
}
}

```


```{r characteristicmodinclude, echo=FALSE, results="asis", message=FALSE, warning=FALSE}
cat(paste(model_traits, collapse = '\n'))
```


# Section 5: Plackett-Luce models with covariates  

## Overall performance  

A model-based recursive partitioning method [@Strobl2009] was used to determine which of the explanatory variables, if any, had significant relationships with the rankings. This approach identifies sub-groups in the data for which the rankings of the different varieties are significantly different to each other. Table 5.1 shows the p-values for each of the covariates tested, one-by-one, showing whether or not the covariate could be used to define sub-groups with significantly different rankings. 

\

```{r univar_pval}
kable(uni_sum[,c(5,4)],
      caption = paste0("Table 5.1. Univariate p-values for first split in ",
                       "Plackett-Luce tree model for the overall ranking."))
```

\

Figure 5.1 shows the partitioning of the rankings based on the most significantly different sub-groups which could be identified from the data using a `sig_level*100`% significance level. At the top of the tree is the full dataset, then working down through the different levels of the tree shows the combinations of variables which define each subgroup. The model parameters are shown for the final subgroups ("terminal nodes") in the plots at the bottoms of the tree. 


```{r pltree, dpi=dpi, asis=TRUE,fig.height=8,fig.width=10, fig.cap="Figure 5.1. Plackett-Luce tree for the overall performance."}
if (length(tree_f)==1) {
  nofig1 <- "No significant explanatory variables identified."
}else{
  nofig1 <- ""
}

plot(tree_f)
```
`r nofig1`

The highest and lowest performing `r option` within each sub-group is identified within Table 5.2.

```{r node_summary}
if (length(siglist) > 0) {
  kable(node_summary,
        caption = "Table 5.2. Summary of performance in each tree node.")
}
```

The model coefficient estimates, along with `r ci_level*100`% confidence intervals are provided in Figure 5.2. This will help identification of which `r options` were better suited to particular sub-groups identified by the analysis.

```{r,dpi=dpi, fig.width=8,fig.height=8, fig.cap="Figure 5.2. Coefficient estimates within each identified terminal node."}
if(length(tree_f)>1){
ggplot(data = coefs_t, 
       aes(x = term, 
           y = ctd,
           ymax = ctd+1.40*quasiSE,
           ymin = ctd-1.40*quasiSE,
           col = Label)) +
  geom_point(position = position_dodge(width = 0.3), size = 1) +
  geom_errorbar(position = position_dodge(width = 0.3), width = 0) +
  coord_flip() +
  scale_color_discrete() +
  geom_text(aes(label=.group),
            size = 3,
            fontface = 2,
            nudge_x = rep(c(-0.3,0.5), each = nlevels(coefs_t$term))) +
  ylab("") +
  xlab(Option) + 
    theme_minimal()
}

```
`r nofig1`

Table 5.3 outlines the p-values for each covariate at each of the nodes in the tree, outlining whether an additional significant split could be determined from within the existing sub-group at that node.

```{r}
z<-NULL
for(i in 1:length(outtabs)){
  if (ncol(outtabs[[i]]) > 3) {
    ot <- data.frame(Variable = rownames(outtabs[[i]]),
                     outtabs[[i]])
    z <- rbind(z, ot)
  }
}
if (length(z) > 0) {
  nodemessage <- ""
  kable(z[,c(1:3,5)],
        caption="Table 5.3: p-values for effect of each covariate at each node.",
        row.names = FALSE)
}
if (length(z)==0) {
  nodemessage <- "No covariates were able to be assessed statistically."
}

```
`r nodemessage`


# References